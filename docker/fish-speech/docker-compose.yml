# ============================================================
# Fish Speech V1.5 — Docker Compose
# Target: NVIDIA DGX Spark (GB10 sm_121, ARM64/aarch64)
#
# Services:
#   fish-speech-api   → REST API server  (host port 8082 → container 8080)
#   fish-speech-webui → Gradio WebUI     (port 7860)
#
# Usage:
#   # API server only (default for DR_2_Podcast integration):
#   docker compose up fish-speech-api
#
#   # WebUI for testing reference audio and prompts:
#   docker compose up fish-speech-webui
#
#   # Both:
#   docker compose up
# ============================================================

name: fish-speech

# ── Shared configuration (YAML anchor) ──────────────────────
x-fish-base: &fish-base
  image: fish-speech:v1.5-arm64
  build:
    context: .
    dockerfile: Dockerfile
    args:
      FISH_SPEECH_TAG: "v1.5.0"
  restart: unless-stopped
  # GPU: allocate ALL GPUs with full access
  # The GB10 (sm_121) requires driver ≥570 and CUDA 12.7+
  # which are both provided by the NGC base image.
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
  environment:
    # Model checkpoint path (must match volume mount target below)
    CHECKPOINTS_PATH: /app/checkpoints/fish-speech-1.5
    # HuggingFace cache directory (persisted in named volume)
    HF_HOME: /app/hf_cache
    # Workers: 1 is enough for a single GPU; increase if batching
    API_WORKERS: "1"
    # PyTorch memory management
    PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
    # Blackwell tuning: disable TF32 for strict reproducibility,
    # or leave enabled (default) for ~10% speed boost on matmuls.
    # TORCH_ALLOW_TF32_CUBLAS_OVERRIDE: "1"
    # Suppress tokenizer fork warnings
    TOKENIZERS_PARALLELISM: "false"
    # HuggingFace token for gated models (optional; add to .env file)
    HF_TOKEN: "${HF_TOKEN:-}"
  volumes:
    # ── Model weights (host ↔ container) ──────────────────
    # Downloaded once on the host, reused on every container start.
    # Host path: ./checkpoints/fish-speech-1.5/
    # Container path: /app/checkpoints/fish-speech-1.5/
    - ./checkpoints/fish-speech-1.5:/app/checkpoints/fish-speech-1.5:ro
    # ── HuggingFace cache (named volume, survives rebuilds) ──
    - hf_cache:/app/hf_cache
    # ── Reference audio scratch directory ─────────────────
    # Mount a host directory here to pass reference .wav files
    # into the container for voice cloning requests.
    - ./reference_audio:/app/reference_audio
    # ── Output directory ───────────────────────────────────
    - ./outputs:/app/outputs
  # Resource limits (unified memory on DGX Spark — tune as needed)
  # DGX Spark has 128GB unified LPDDR5x; the container sees standard
  # GPU VRAM (the GB10 allocates from the shared pool).
  # Uncomment to cap container memory:
  # mem_limit: 32g

services:
  # ── API Server ───────────────────────────────────────────
  # Primary service for DR_2_Podcast TTS integration.
  # Exposes an OpenAI-compatible /v1/tts endpoint.
  fish-speech-api:
    <<: *fish-base
    container_name: fish-speech-api
    environment:
      CHECKPOINTS_PATH: /app/checkpoints/fish-speech-1.5
      HF_HOME: /app/hf_cache
      API_WORKERS: "1"
      API_PORT: "8080"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
      TOKENIZERS_PARALLELISM: "false"
      HF_TOKEN: "${HF_TOKEN:-}"
      MODE: "api"
    ports:
      # host:container
      # SearXNG already uses 8080 on this host — Fish Speech binds to 8082
      - "8082:8080"
    command: ["api"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # ── WebUI (Gradio) ───────────────────────────────────────
  # Use this for testing reference audio, adjusting speaker
  # style, and verifying Japanese output quality.
  fish-speech-webui:
    <<: *fish-base
    container_name: fish-speech-webui
    environment:
      CHECKPOINTS_PATH: /app/checkpoints/fish-speech-1.5
      HF_HOME: /app/hf_cache
      WEBUI_PORT: "7860"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
      TOKENIZERS_PARALLELISM: "false"
      HF_TOKEN: "${HF_TOKEN:-}"
      MODE: "webui"
    ports:
      - "7860:7860"
    command: ["webui"]
    profiles:
      # Not started by default with `docker compose up`.
      # Start explicitly: docker compose --profile webui up fish-speech-webui
      - webui
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

# ── Named volumes ────────────────────────────────────────────
volumes:
  hf_cache:
    driver: local
